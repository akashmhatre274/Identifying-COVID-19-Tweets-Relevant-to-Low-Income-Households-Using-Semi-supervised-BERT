{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c3a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# Unique to this file\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "from train_test import get_device, compute_accuracy\n",
    "\n",
    "\n",
    "# Depending on your GPU you can either increase or decrease this value\n",
    "batch_size = 16\n",
    "total_epoch = 10\n",
    "learning_rate = 1e-5\n",
    "iter_num = 1\n",
    "\n",
    "# Find out how many labels are in the dataset\n",
    "with open('covid_dataset/5_class_map.pkl','rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "labels_in_dst = len(labels)\n",
    "\n",
    "\n",
    "\n",
    "model_map= {\n",
    "    #\"tinybert\": \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "    #\"covidbert\": \"digitalepidemiologylab/covid-twitter-bert-v2\",\n",
    "    #\"distilbert\": \"distilbert-base-uncased\",\n",
    "    #\"bertweet\": \"vinai/bertweet-base\",\n",
    "    \"bertweetcovid\": \"vinai/bertweet-covid19-base-uncased\"\n",
    "}\n",
    "# train teacher model for self training\n",
    "# the train_st.dst is TensorDataset that assign each tweet a weight 1, that's the only difference with train.dst\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "dst_path = 'preprocessed_data/{}.dst'\n",
    "dst_path = os.path.join(cwd,'preprocessed_data/{}-{{}}.dst'.format(\"bertweetcovid\"))\n",
    "\n",
    "train = torch.load(dst_path.format('train'))\n",
    "val = torch.load(dst_path.format('val'))\n",
    "test = torch.load(dst_path.format('test'))\n",
    "unlabeled = torch.load(dst_path.format('19k'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2a5ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19591\n"
     ]
    }
   ],
   "source": [
    "input_ids, masks, _ = zip(*unlabeled)\n",
    "\n",
    "input_ids = torch.stack(input_ids)\n",
    "masks = torch.stack(masks)\n",
    "\n",
    "df_19k = pd.read_csv('covid_dataset/splits/19k_bertweetcovid_probs.csv')\n",
    "\n",
    "pred_probs = df_19k[['class0', 'class1', 'class2', 'class3', 'class4']].to_numpy()\n",
    "\n",
    "paper_pred_prob = torch.tensor(pred_probs, dtype=torch.float32)\n",
    "\n",
    "unlabeled_prob = TensorDataset(input_ids, masks, paper_pred_prob )\n",
    "print(unlabeled_prob.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce7ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss for two probability distribution\n",
    "# https://discuss.pytorch.org/t/how-should-i-implement-cross-entropy-loss-with-continuous-target-outputs/10720/18\n",
    "def cross_entropy(pred, soft_targets):\n",
    "    logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "\n",
    "#assuming pred and soft_targets are both Variables with shape (batchsize, num_of_classes), \n",
    "#each row of pred is predicted logits and each row of soft_targets is a discrete distribution.\n",
    "\n",
    "\n",
    "# class CrossEntropyLossForSoftTarget(nn.Module):\n",
    "#     def __init__(self, T=20):\n",
    "#         super(CrossEntropyLossForSoftTarget, self).__init__()\n",
    "#         self.T = T\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "#         self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "#     def forward(self, y_pred, y_gt):\n",
    "#         y_pred_soft = y_pred.div(self.T)\n",
    "#         y_gt_soft = y_gt.div(self.T)\n",
    "#         return -(y_gt_soft)*self.logsoftmax(y_pred_soft).mean().mul(self.T*self.T)\n",
    "\n",
    "class CrossEntropyLossForSoftTarget(nn.Module):\n",
    "    '''\n",
    "    y_pred: logits from student model\n",
    "    y_gt:   logits from teacher model\n",
    "    '''\n",
    "    def __init__(self, T=20):\n",
    "        super(CrossEntropyLossForSoftTarget, self).__init__()\n",
    "        self.T = T\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, y_pred, y_gt):\n",
    "        y_gt_soft = y_gt.div(self.T)\n",
    "        y_pred_soft = y_pred.div(self.T)\n",
    "#         return (-(y_gt_soft)*self.logsoftmax(y_pred_soft)).mean().mul(self.T*self.T)\n",
    "        return torch.mean(torch.sum(- self.softmax(y_gt_soft) * self.logsoftmax(y_pred_soft), 1)).mul(self.T*self.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f7eb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def train_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe807561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distill_student(teacher_path, student_path, train, unlabel, val, test, batch_size, batch_size_unlabel, total_epoch, labels_in_dst, learning_rate, T=10, unlabel_iter=None):\n",
    "    global model_map\n",
    "\n",
    "    trainloader = DataLoader(train, shuffle=True, batch_size=batch_size)\n",
    "    valloader = DataLoader(val, shuffle=False, batch_size=batch_size)\n",
    "    unlabel_loader = DataLoader(unlabel, shuffle=True, batch_size=batch_size_unlabel)\n",
    "    \n",
    "    \n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    teacher = AutoModelForSequenceClassification.from_pretrained(model_map['bertweetcovid'], \n",
    "                                                          num_labels=labels_in_dst,\n",
    "                                                          return_dict=True)\n",
    "    \n",
    "    checkpoint = torch.load(teacher_path)\n",
    "    teacher.load_state_dict(checkpoint['model_state_dict'])\n",
    "    teacher = teacher.to(device)\n",
    "    teacher.eval()\n",
    "    \n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_map['bertweetcovid'], \n",
    "                                                          num_labels=labels_in_dst,\n",
    "                                                          return_dict=True)\n",
    "    \n",
    "#     checkpoint = torch.load(student_path)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    save_model_path = \"student_\" + student_path\n",
    "    model = model.to(device)\n",
    "    gc.collect()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    sum_loss = []\n",
    "    sum_val = []\n",
    "    \n",
    "    val_f1_average = []\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(0, total_epoch):\n",
    "        print('Epoch:', epoch)\n",
    "        \n",
    "        train_loss = []\n",
    "        soft_all_loss = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        \n",
    "\n",
    "        get_next_fnt = iter(unlabel_loader) # get one mini batch at a time\n",
    "        unlabel_num_batchs = len(unlabel) // batch_size_unlabel + 1\n",
    "        print(unlabel_num_batchs)\n",
    "        \n",
    "        step = 0\n",
    "        total_train_accuracy = 0\n",
    "        ## miniminze loss on labeled train per minibatch\n",
    "        for input_ids, attention_mask, labels in trainloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output1 = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = output1.logits\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            b_accu = train_accuracy(logits, label_ids)\n",
    "            total_train_accuracy += b_accu\n",
    "            \n",
    "        \n",
    "            # minimize soft loss on unlabeled per mini batch,batch size is train batch size * 2\n",
    "            unlabel_input_ids, unlabel_attention_mask, unlabel_labels = next(get_next_fnt)\n",
    "            unlabel_input_ids = unlabel_input_ids.to(device)\n",
    "            unlabel_attention_mask = unlabel_attention_mask.to(device)\n",
    "            unlabel_labels = unlabel_labels.to(device) # teacher predicted probabilities\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output2 = model(unlabel_input_ids, attention_mask=unlabel_attention_mask)\n",
    "\n",
    "            logits2 = output2.logits\n",
    "            \n",
    "            teacher_output = teacher(unlabel_input_ids, attention_mask=unlabel_attention_mask)\n",
    "\n",
    "            loss_fnt = CrossEntropyLossForSoftTarget(T) # T temprature, to smooth probabilities\n",
    "\n",
    "            soft_loss = loss_fnt(logits2, teacher_output.logits)\n",
    "\n",
    "\n",
    "            soft_loss.backward()\n",
    "            optimizer.step()\n",
    "            soft_all_loss.append(soft_loss.item())\n",
    "            \n",
    "            step += 1\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(trainloader), elapsed))\n",
    "\n",
    "        # Report the final accuracy for this  run.\n",
    "        avg_train_accuracy = total_train_accuracy / len(trainloader)\n",
    "        print(\"Train Accuracy: {0:.4f}\".format(avg_train_accuracy))\n",
    "        \n",
    "        \n",
    "        # continue minimize soft loss on all rest of unlabeled\n",
    "        unlabel_remain_batchs = unlabel_num_batchs - step\n",
    "        \n",
    "        # None is default, will use all unlabeled data, \n",
    "        # if 100 us passed then only 100 additional mini-batches of unlabeled data will be used.\n",
    "        if unlabel_iter is not None: \n",
    "            unlabel_remain_batchs = unlabel_iter\n",
    "            \n",
    "        \n",
    "        for i in range(unlabel_remain_batchs):\n",
    "            try:\n",
    "                unlabel_input_ids, unlabel_attention_mask, unlabel_labels = next(get_next_fnt)\n",
    "            except StopIteration:\n",
    "                get_next_fnt = iter(unlabel_loader) # get one mini batch at a time\n",
    "                unlabel_input_ids, unlabel_attention_mask, unlabel_labels = next(get_next_fnt)\n",
    "            \n",
    "            unlabel_input_ids = unlabel_input_ids.to(device)\n",
    "            unlabel_attention_mask = unlabel_attention_mask.to(device)\n",
    "            unlabel_labels = unlabel_labels.to(device) # teacher predicted probabilities\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output2 = model(unlabel_input_ids, attention_mask=unlabel_attention_mask)\n",
    "\n",
    "            logits2 = output2.logits\n",
    "            \n",
    "            teacher_output = teacher(unlabel_input_ids, attention_mask=unlabel_attention_mask)\n",
    "\n",
    "            loss_fnt = CrossEntropyLossForSoftTarget(T) # T temprature, to smooth probabilities\n",
    "            soft_loss = loss_fnt(logits2, teacher_output.logits)\n",
    "  \n",
    "\n",
    "            soft_loss.backward()\n",
    "            optimizer.step()\n",
    "            soft_all_loss.append(soft_loss.item())\n",
    "            \n",
    "            if i % 40 == 0 and not i == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i+step, len(unlabel_loader), elapsed))\n",
    "        average_train_loss = np.sum(train_loss)/len(train)\n",
    "        average_soft_loss = np.sum(soft_all_loss)/len(unlabel)\n",
    "        print('Train labeled Loss: {:.4f}'.format(average_train_loss))\n",
    "        print('Soft labeled Loss: {:.4f}'.format(average_soft_loss))\n",
    "    \n",
    "        sum_loss.append(average_train_loss + average_soft_loss)  \n",
    "        print('Total Loss: {:.4f}'.format(sum_loss[epoch-1]))\n",
    "\n",
    "#       evaluation part \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for input_ids, attention_mask, labels in valloader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model(input_ids, attention_mask=attention_mask)\n",
    "                predictions.append(output.logits.clone().detach())\n",
    "                true_labels.append(labels.clone().detach())\n",
    "            predictions = torch.cat(predictions)\n",
    "            true_labels = torch.cat(true_labels)\n",
    "            predictions = predictions.cpu()\n",
    "            true_labels = true_labels.cpu()\n",
    "\n",
    "            # val_f1 is weighted f1 \n",
    "            acc, precision, recall, f1_macro, val_f1  = compute_accuracy(predictions, true_labels)\n",
    "            print(\"validation performance at epoch: \", epoch, acc, precision, recall, f1_macro, val_f1)\n",
    "            \n",
    "            \n",
    "            best_f1 = max(val_f1_average, default=-1)\n",
    "            best_model_state = ''\n",
    "            # Save the best model seen so far\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_f1': best_f1\n",
    "                            }, save_model_path)\n",
    "#                 torch.save(model, save_model_path)\n",
    "            \n",
    "            val_f1_average.append(val_f1)\n",
    "    \n",
    "        # test\n",
    "        model.eval()\n",
    "        testloader = DataLoader(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            pred_prob = []\n",
    "            for input_ids, attention_mask, labels in testloader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                output = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = output.logits.clone().detach()\n",
    "\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(labels.clone().detach())\n",
    "\n",
    "                softmax = torch.nn.Softmax(dim=1)\n",
    "                prob_batch = softmax(logits)\n",
    "                prob_batch = prob_batch.cpu().numpy()\n",
    "                pred_prob.append(prob_batch)\n",
    "\n",
    "            predictions = torch.cat(predictions)\n",
    "            true_labels = torch.cat(true_labels)\n",
    "            predictions = predictions.cpu()\n",
    "            true_labels = true_labels.cpu()\n",
    "\n",
    "            flat_prob = np.concatenate(pred_prob, axis=0)\n",
    "\n",
    "            pred_labels = np.argmax(flat_prob, axis=1).flatten()\n",
    "\n",
    "            acc, precision, recall,f1_macro, f1_score  = compute_accuracy(predictions, true_labels)\n",
    "\n",
    "            print(\"test model performance at epoch : \", epoch, acc, precision, recall,f1_macro, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf782993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:39.\n",
      "Train Accuracy: 0.5969\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:06.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:43.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:21.\n",
      "  Batch   289  of    613.    Elapsed: 0:04:58.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:35.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:12.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:50.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:27.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:05.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:42.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:20.\n",
      "  Batch   609  of    613.    Elapsed: 0:09:58.\n",
      "Train labeled Loss: 0.0711\n",
      "Soft labeled Loss: 0.0198\n",
      "Total Loss: 0.0910\n",
      "validation performance at epoch:  0 tensor(0.8407) 0.8033248252506547 0.8076860391831243 0.8000721629782517 0.8374656995772355\n",
      "test model performance at epoch :  0 tensor(0.7805) 0.748779953243717 0.7309508301178803 0.7379787411211062 0.7785459413548097\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:51.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8336\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:44.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:21.\n",
      "  Batch   289  of    613.    Elapsed: 0:04:58.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:35.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:13.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:50.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:27.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:04.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:41.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:19.\n",
      "  Batch   609  of    613.    Elapsed: 0:09:56.\n",
      "Train labeled Loss: 0.0305\n",
      "Soft labeled Loss: 0.0134\n",
      "Total Loss: 0.0910\n",
      "validation performance at epoch:  1 tensor(0.8358) 0.7998604306918914 0.7714399749548919 0.7809165842715828 0.8325243206480732\n",
      "test model performance at epoch :  1 tensor(0.7927) 0.8069480470507113 0.7309984677080859 0.7568675086922652 0.7898958370532911\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:49.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:38.\n",
      "Train Accuracy: 0.9108\n",
      "  Batch   129  of    613.    Elapsed: 0:02:27.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:04.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:41.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:19.\n",
      "  Batch   289  of    613.    Elapsed: 0:04:57.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:34.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:12.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:50.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:27.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:05.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:43.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:20.\n",
      "  Batch   609  of    613.    Elapsed: 0:09:58.\n",
      "Train labeled Loss: 0.0176\n",
      "Soft labeled Loss: 0.0123\n",
      "Total Loss: 0.0439\n",
      "validation performance at epoch:  2 tensor(0.8333) 0.7849765958880484 0.7689414504385356 0.7730824004924679 0.8296855547456505\n",
      "test model performance at epoch :  2 tensor(0.7764) 0.7570797346664705 0.7336337569471485 0.7434490096433831 0.7749850744629612\n"
     ]
    }
   ],
   "source": [
    "total_epoch = 3\n",
    "batch_size = 16\n",
    "batch_size_unlabel = 16 * 2\n",
    "T = 1\n",
    "unlabel_iter = None # use all unlabeled data\n",
    "model_name = \"paper_student_distillv2_errortrial_epoch\" + str(total_epoch) + \"_batchunlabel\" + str(batch_size_unlabel) + \"_T\" + str(T) + \"_unlabeliter\" + str(unlabel_iter)\n",
    "\n",
    "\n",
    "train_distill_student(\"bertweetcovid_paper.pth\", model_name, train, unlabeled_prob, val, test, batch_size, batch_size_unlabel, total_epoch, labels_in_dst, learning_rate, T, unlabel_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45bcf681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:51.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.5590\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:44.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:22.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:00.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:37.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:15.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:52.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:29.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:06.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:43.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:21.\n",
      "  Batch   609  of    613.    Elapsed: 0:09:58.\n",
      "Train labeled Loss: 0.0741\n",
      "Soft labeled Loss: 0.0205\n",
      "Total Loss: 0.0946\n",
      "validation performance at epoch:  0 tensor(0.8358) 0.7840564990564991 0.7772699562122968 0.776386993539179 0.832122829400619\n",
      "test model performance at epoch :  0 tensor(0.7764) 0.7453170359052712 0.6953837556828814 0.7132256049084434 0.7713395721220164\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:49.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:37.\n",
      "Train Accuracy: 0.8188\n",
      "  Batch   129  of    613.    Elapsed: 0:02:26.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:03.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:40.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:17.\n",
      "  Batch   289  of    613.    Elapsed: 0:04:55.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:32.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:09.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:46.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:23.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:00.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:37.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:14.\n",
      "  Batch   609  of    613.    Elapsed: 0:09:51.\n",
      "Train labeled Loss: 0.0319\n",
      "Soft labeled Loss: 0.0135\n",
      "Total Loss: 0.0946\n",
      "validation performance at epoch:  1 tensor(0.8431) 0.797496895395338 0.7924239290305032 0.7908985095271991 0.8401277928387337\n",
      "test model performance at epoch :  1 tensor(0.7967) 0.7766146134567187 0.7657375482317959 0.7676645026351484 0.7952511474451557\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:49.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:38.\n",
      "Train Accuracy: 0.8790\n",
      "  Batch   129  of    613.    Elapsed: 0:02:27.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:04.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:41.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:18.\n",
      "  Batch   289  of    613.    Elapsed: 0:04:56.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:33.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:10.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:47.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:24.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:02.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:39.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:16.\n",
      "  Batch   609  of    613.    Elapsed: 0:09:53.\n",
      "Train labeled Loss: 0.0208\n",
      "Soft labeled Loss: 0.0125\n",
      "Total Loss: 0.0454\n",
      "validation performance at epoch:  2 tensor(0.8382) 0.8032892126296696 0.79042531936766 0.7909213350895836 0.8347112190324518\n",
      "test model performance at epoch :  2 tensor(0.8008) 0.7795788660291703 0.7712079315070571 0.7715315563380543 0.798991471871261\n",
      "Epoch: 3\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:49.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:38.\n",
      "Train Accuracy: 0.9396\n",
      "  Batch   129  of    613.    Elapsed: 0:02:28.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:05.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:42.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:19.\n",
      "  Batch   289  of    613.    Elapsed: 0:04:56.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:33.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:10.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:47.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:24.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:02.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:40.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:17.\n",
      "  Batch   609  of    613.    Elapsed: 0:09:55.\n",
      "Train labeled Loss: 0.0115\n",
      "Soft labeled Loss: 0.0121\n",
      "Total Loss: 0.0333\n",
      "validation performance at epoch:  3 tensor(0.8456) 0.8127924426756143 0.8115268455786753 0.8061282356879191 0.8425775791918705\n",
      "test model performance at epoch :  3 tensor(0.8049) 0.7974650063894915 0.7929688535118815 0.7930648921679931 0.8043481540479337\n",
      "Epoch: 4\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9698\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:38.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:54.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:25.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0079\n",
      "Soft labeled Loss: 0.0118\n",
      "Total Loss: 0.0235\n",
      "validation performance at epoch:  4 tensor(0.8431) 0.8126058239454466 0.8150934465905317 0.8083840432724813 0.8401110462966646\n",
      "test model performance at epoch :  4 tensor(0.7967) 0.7767547294105949 0.783331091366066 0.7766852922040645 0.7953025280759907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:53.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:43.\n",
      "Train Accuracy: 0.5969\n",
      "  Batch   129  of    613.    Elapsed: 0:02:33.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:10.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:48.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:26.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:04.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:42.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:20.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:58.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:36.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:14.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:52.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:30.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:08.\n",
      "Train labeled Loss: 0.0700\n",
      "Soft labeled Loss: 0.0196\n",
      "Total Loss: 0.0896\n",
      "validation performance at epoch:  0 tensor(0.8456) 0.8153807140951995 0.7987106404267037 0.8000248226950355 0.8425776578361842\n",
      "test model performance at epoch :  0 tensor(0.7805) 0.790892958892959 0.6970424237520418 0.7242378665091911 0.7748914951269549\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:51.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8340\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:54.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:47.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:25.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0299\n",
      "Soft labeled Loss: 0.0134\n",
      "Total Loss: 0.0896\n",
      "validation performance at epoch:  1 tensor(0.8358) 0.8004250651617347 0.7889654653530614 0.7884559267470845 0.8319445423817317\n",
      "test model performance at epoch :  1 tensor(0.8049) 0.7923648002868582 0.7779326701830154 0.7830120424238072 0.8033838648377146\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:39.\n",
      "Train Accuracy: 0.9134\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:00.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:38.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:54.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:25.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0176\n",
      "Soft labeled Loss: 0.0123\n",
      "Total Loss: 0.0433\n",
      "validation performance at epoch:  2 tensor(0.8284) 0.7794413388371126 0.7769137253191533 0.77238554331296 0.8252160541235508\n",
      "test model performance at epoch :  2 tensor(0.7764) 0.7380553340687908 0.7323876445689608 0.7329773676115139 0.7746942064015235\n",
      "Epoch: 3\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9586\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:18.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:56.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:34.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0099\n",
      "Soft labeled Loss: 0.0120\n",
      "Total Loss: 0.0299\n",
      "validation performance at epoch:  3 tensor(0.8309) 0.7956091771513792 0.7844387048619644 0.7854640206029389 0.8278129873370347\n",
      "test model performance at epoch :  3 tensor(0.7927) 0.7784615384615385 0.7580079193700915 0.7657864158503428 0.7921168460289373\n",
      "Epoch: 4\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:39.\n",
      "Train Accuracy: 0.9775\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:22.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:00.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:37.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:15.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:53.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:31.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:08.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:46.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:24.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:02.\n",
      "Train labeled Loss: 0.0065\n",
      "Soft labeled Loss: 0.0117\n",
      "Total Loss: 0.0218\n",
      "validation performance at epoch:  4 tensor(0.8284) 0.7897933157902881 0.7836648049429119 0.780490560971842 0.8241811946903649\n",
      "test model performance at epoch :  4 tensor(0.7805) 0.7606064195812693 0.7323655957480164 0.7423297295895879 0.7773803478876353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:53.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:42.\n",
      "Train Accuracy: 0.6011\n",
      "  Batch   129  of    613.    Elapsed: 0:02:32.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:10.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:47.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:25.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:03.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:41.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:19.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:57.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:34.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:12.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:50.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:28.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0706\n",
      "Soft labeled Loss: 0.0199\n",
      "Total Loss: 0.0905\n",
      "validation performance at epoch:  0 tensor(0.8309) 0.7971451763630534 0.7782560071983478 0.7799081687739078 0.8278067539326468\n",
      "test model performance at epoch :  0 tensor(0.7642) 0.7382296145883103 0.7143201023550771 0.7225885810644487 0.7611001053602784\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8404\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0294\n",
      "Soft labeled Loss: 0.0134\n",
      "Total Loss: 0.0905\n",
      "validation performance at epoch:  1 tensor(0.8358) 0.8048064917127495 0.8039628049229772 0.7989375924282756 0.8327139261501703\n",
      "test model performance at epoch :  1 tensor(0.7805) 0.7652861987145665 0.7284177948144814 0.7424019253919913 0.7780885258894005\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:39.\n",
      "Train Accuracy: 0.9003\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:22.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:00.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:38.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:53.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:31.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:09.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:47.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:24.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:02.\n",
      "Train labeled Loss: 0.0176\n",
      "Soft labeled Loss: 0.0124\n",
      "Total Loss: 0.0428\n",
      "validation performance at epoch:  2 tensor(0.8358) 0.7993376438000575 0.7871232686203538 0.7881391073047048 0.8325094831716731\n",
      "test model performance at epoch :  2 tensor(0.7724) 0.7474660809590802 0.7182491415625332 0.7301976116788051 0.7707090953249546\n",
      "Epoch: 3\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9551\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:54.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:04.\n",
      "Train labeled Loss: 0.0102\n",
      "Soft labeled Loss: 0.0119\n",
      "Total Loss: 0.0300\n",
      "validation performance at epoch:  3 tensor(0.8382) 0.8086880510793554 0.8071472835348796 0.8013918888856649 0.8344664828567684\n",
      "test model performance at epoch :  3 tensor(0.7846) 0.768008658008658 0.759699154963766 0.7609175892650364 0.7834201741906756\n",
      "Epoch: 4\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9796\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0060\n",
      "Soft labeled Loss: 0.0117\n",
      "Total Loss: 0.0221\n",
      "validation performance at epoch:  4 tensor(0.8358) 0.8063321810979656 0.8029241344212196 0.7988900179071694 0.832774654135126\n",
      "test model performance at epoch :  4 tensor(0.8049) 0.7837201611174215 0.7769360230197783 0.7773820396470998 0.8040426357852158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:52.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:41.\n",
      "Train Accuracy: 0.6016\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:09.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:18.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:56.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:34.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:12.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:50.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:28.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0699\n",
      "Soft labeled Loss: 0.0198\n",
      "Total Loss: 0.0897\n",
      "validation performance at epoch:  0 tensor(0.8333) 0.800781529420368 0.7914113703537109 0.7882765590167627 0.8294675518497188\n",
      "test model performance at epoch :  0 tensor(0.7927) 0.779245134965474 0.7707040451495122 0.76637340993006 0.7902454315555331\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8265\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:18.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:56.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0303\n",
      "Soft labeled Loss: 0.0134\n",
      "Total Loss: 0.0897\n",
      "validation performance at epoch:  1 tensor(0.8358) 0.8067854015086402 0.8266323224830059 0.8106065182751039 0.8323765131608124\n",
      "test model performance at epoch :  1 tensor(0.7927) 0.7595354449472096 0.7803937433942035 0.7668011881111696 0.7922618105838914\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9143\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:54.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:47.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:25.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0172\n",
      "Soft labeled Loss: 0.0124\n",
      "Total Loss: 0.0436\n",
      "validation performance at epoch:  2 tensor(0.8284) 0.796127154196095 0.7845859033092658 0.7819701012631326 0.8242509968090459\n",
      "test model performance at epoch :  2 tensor(0.7967) 0.7709403184575349 0.7746712146574088 0.7716753036182451 0.7957145197908377\n",
      "Epoch: 3\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:39.\n",
      "Train Accuracy: 0.9492\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:44.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:22.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:00.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:38.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:53.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:31.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:09.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:47.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:25.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0109\n",
      "Soft labeled Loss: 0.0120\n",
      "Total Loss: 0.0296\n",
      "validation performance at epoch:  3 tensor(0.8407) 0.8028219441377337 0.7971467726795214 0.7961494662290194 0.8380845512226657\n",
      "test model performance at epoch :  3 tensor(0.7927) 0.7610705154229717 0.7484316006129168 0.7542911068827001 0.7920393114096624\n",
      "Epoch: 4\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9775\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0069\n",
      "Soft labeled Loss: 0.0117\n",
      "Total Loss: 0.0229\n",
      "validation performance at epoch:  4 tensor(0.8358) 0.7977266330371461 0.8048839032893312 0.793865335915992 0.8328753413930666\n",
      "test model performance at epoch :  4 tensor(0.7846) 0.7641759754120725 0.7666063526901079 0.760911333282516 0.7823011327654396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:52.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:42.\n",
      "Train Accuracy: 0.5890\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:09.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:04.\n",
      "Train labeled Loss: 0.0714\n",
      "Soft labeled Loss: 0.0198\n",
      "Total Loss: 0.0912\n",
      "validation performance at epoch:  0 tensor(0.8358) 0.7952876761191369 0.7881619391221115 0.7874453255653717 0.8325861267829522\n",
      "test model performance at epoch :  0 tensor(0.7886) 0.7556030790070029 0.734832231735131 0.743073353619381 0.7866368266855273\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8284\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:04.\n",
      "Train labeled Loss: 0.0307\n",
      "Soft labeled Loss: 0.0134\n",
      "Total Loss: 0.0912\n",
      "validation performance at epoch:  1 tensor(0.8358) 0.7995192159272045 0.7872408407557577 0.7887883544446911 0.8328487750778094\n",
      "test model performance at epoch :  1 tensor(0.7724) 0.7516991184305415 0.7169145810471167 0.7310473267422768 0.7708625261083042\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9092\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0180\n",
      "Soft labeled Loss: 0.0123\n",
      "Total Loss: 0.0441\n",
      "validation performance at epoch:  2 tensor(0.8382) 0.814769907511843 0.8071472835348796 0.8036556541760647 0.8345159171142926\n",
      "test model performance at epoch :  2 tensor(0.7927) 0.7846410505154662 0.7706155970122835 0.7758790701939255 0.7914937656440001\n",
      "Epoch: 3\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:39.\n",
      "Train Accuracy: 0.9508\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:44.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:22.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:00.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:38.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:15.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:53.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:31.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:09.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:47.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:24.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:02.\n",
      "Train labeled Loss: 0.0103\n",
      "Soft labeled Loss: 0.0120\n",
      "Total Loss: 0.0304\n",
      "validation performance at epoch:  3 tensor(0.8407) 0.8096991470668845 0.8086071375494782 0.8022562075183798 0.8372906432999057\n",
      "test model performance at epoch :  3 tensor(0.7927) 0.773944421232007 0.7789375603687615 0.7728541827830433 0.7902369054898668\n",
      "Epoch: 4\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9796\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0067\n",
      "Soft labeled Loss: 0.0118\n",
      "Total Loss: 0.0223\n",
      "validation performance at epoch:  4 tensor(0.8358) 0.8121878274426999 0.8196460985884393 0.8093098680767854 0.8324808465896479\n",
      "test model performance at epoch :  4 tensor(0.7805) 0.7572170597295146 0.7704877543073586 0.7596589700471843 0.7794562973845405\n"
     ]
    }
   ],
   "source": [
    "total_epoch = 5\n",
    "batch_size = 16\n",
    "batch_size_unlabel = 16 * 2\n",
    "T = 1 # temperature\n",
    "unlabel_iter = None\n",
    "model_name = \"paper_student_distillv2_errortrial_epoch\" + str(total_epoch) + \"_batchunlabel\" + str(batch_size_unlabel) + \"_T\" + str(T) + \"_unlabeliter\" + str(unlabel_iter)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    train_distill_student(\"bertweetcovid_paper.pth\", str(i)+model_name, train, unlabeled_prob, val, test, batch_size, batch_size_unlabel, total_epoch, labels_in_dst, learning_rate, T, unlabel_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57430395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7805) 0.758503526150585 0.722299196431732 0.7336381500446072 0.7774962922731367\n"
     ]
    }
   ],
   "source": [
    "_, _ = test_ST(\"student_4paper_student_distillv2_errortrial_epoch3_batchunlabel32_T10_unlabeliterNone\", test,labels_in_dst, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3878399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:53.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:43.\n",
      "Train Accuracy: 0.5941\n",
      "  Batch   129  of    613.    Elapsed: 0:02:33.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:11.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:49.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:26.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:04.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:42.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:19.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:57.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:35.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:13.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:50.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:28.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:06.\n",
      "Train labeled Loss: 0.0700\n",
      "Soft labeled Loss: 4.9743\n",
      "Total Loss: 5.0443\n",
      "validation performance at epoch:  0 tensor(0.8407) 0.8157270497847747 0.8028947284274771 0.8042081154809088 0.8375303880774996\n",
      "test model performance at epoch :  0 tensor(0.7724) 0.7890013495276652 0.7131830709557354 0.7375565510160624 0.7705771963624005\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:39.\n",
      "Train Accuracy: 0.8371\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:22.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:00.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:38.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:54.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:31.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:09.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:47.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:25.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0310\n",
      "Soft labeled Loss: 4.9632\n",
      "Total Loss: 5.0443\n",
      "validation performance at epoch:  1 tensor(0.8358) 0.7991901776384536 0.7812932873629488 0.7856280792690981 0.8328260552938049\n",
      "test model performance at epoch :  1 tensor(0.7967) 0.7907218059885938 0.756161229474621 0.7674224978545563 0.7956979475424178\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9066\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0178\n",
      "Soft labeled Loss: 4.9619\n",
      "Total Loss: 4.9942\n",
      "validation performance at epoch:  2 tensor(0.8431) 0.8158796177982225 0.803315911940318 0.8034152400772928 0.8404169515131545\n",
      "test model performance at epoch :  2 tensor(0.8008) 0.8093246484073667 0.7651369707145134 0.7825891007946757 0.7997393866296306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:52.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:41.\n",
      "Train Accuracy: 0.5911\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:09.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:47.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:25.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:18.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:04.\n",
      "Train labeled Loss: 0.0694\n",
      "Soft labeled Loss: 4.9747\n",
      "Total Loss: 5.0441\n",
      "validation performance at epoch:  0 tensor(0.8333) 0.797063492063492 0.7639149954120806 0.7746705544908812 0.8294530187127204\n",
      "test model performance at epoch :  0 tensor(0.7724) 0.7402760777913777 0.7040574381899738 0.716799992735264 0.7689823928342533\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:39.\n",
      "Train Accuracy: 0.8336\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:38.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:54.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0309\n",
      "Soft labeled Loss: 4.9630\n",
      "Total Loss: 5.0441\n",
      "validation performance at epoch:  1 tensor(0.8431) 0.8153849967488698 0.8043545824420757 0.8039009702940738 0.8403824323355256\n",
      "test model performance at epoch :  1 tensor(0.7724) 0.771167056986729 0.7411891697809785 0.7522371141356633 0.7713622096132476\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8993\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:18.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:56.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:34.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:12.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0181\n",
      "Soft labeled Loss: 4.9621\n",
      "Total Loss: 4.9939\n",
      "validation performance at epoch:  2 tensor(0.8358) 0.8061151056307972 0.7872408407557577 0.7917915959090338 0.8328332815042828\n",
      "test model performance at epoch :  2 tensor(0.7846) 0.7660230008111908 0.735480446842619 0.7473261384376366 0.7829333403723646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:52.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:42.\n",
      "Train Accuracy: 0.5962\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:09.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:47.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:25.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:18.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:56.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0697\n",
      "Soft labeled Loss: 4.9746\n",
      "Total Loss: 5.0443\n",
      "validation performance at epoch:  0 tensor(0.8358) 0.8048113144155135 0.805001475424735 0.7991765106091255 0.8328191302367517\n",
      "test model performance at epoch :  0 tensor(0.7764) 0.7704986371653039 0.727313330939654 0.7434597285055983 0.774395714756717\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8284\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:04.\n",
      "Train labeled Loss: 0.0316\n",
      "Soft labeled Loss: 4.9633\n",
      "Total Loss: 5.0443\n",
      "validation performance at epoch:  1 tensor(0.8382) 0.8078271003025048 0.8054226589375759 0.801392457017682 0.8354513217020377\n",
      "test model performance at epoch :  1 tensor(0.7805) 0.7513180608584982 0.7351563392888749 0.7423083972140551 0.7792630882395697\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9010\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:09.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:18.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:56.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:04.\n",
      "Train labeled Loss: 0.0188\n",
      "Soft labeled Loss: 4.9619\n",
      "Total Loss: 4.9949\n",
      "validation performance at epoch:  2 tensor(0.8407) 0.8158751356170711 0.8086071375494782 0.8052032612601987 0.8371935532049706\n",
      "test model performance at epoch :  2 tensor(0.7805) 0.7596914822266936 0.7438162159975321 0.7485307129396774 0.7793409697938826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:52.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:42.\n",
      "Train Accuracy: 0.5627\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:09.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:47.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:25.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:03.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:41.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:18.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:56.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:34.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:12.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:50.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:28.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0721\n",
      "Soft labeled Loss: 4.9748\n",
      "Total Loss: 5.0468\n",
      "validation performance at epoch:  0 tensor(0.8382) 0.8076193661411052 0.8062261851685257 0.8006667686970717 0.8348045467549329\n",
      "test model performance at epoch :  0 tensor(0.7846) 0.7638831394189145 0.7480881244848111 0.7529812495869265 0.7835654702468954\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8399\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:04.\n",
      "Train labeled Loss: 0.0304\n",
      "Soft labeled Loss: 4.9632\n",
      "Total Loss: 5.0468\n",
      "validation performance at epoch:  1 tensor(0.8431) 0.8107475378055042 0.8091458931977229 0.8042011199648107 0.8401687518804574\n",
      "test model performance at epoch :  1 tensor(0.8008) 0.7892857142857143 0.7680881244848111 0.777439292000141 0.7998664869644334\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.9108\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:54.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:32.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:10.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:48.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:26.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:03.\n",
      "Train labeled Loss: 0.0175\n",
      "Soft labeled Loss: 4.9620\n",
      "Total Loss: 4.9936\n",
      "validation performance at epoch:  2 tensor(0.8382) 0.7964455481262112 0.8063437573039296 0.7963594580406174 0.8351964728250604\n",
      "test model performance at epoch :  2 tensor(0.7846) 0.7591269841269842 0.7708118618611026 0.7582367529514544 0.7842270947406605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:52.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:41.\n",
      "Train Accuracy: 0.5658\n",
      "  Batch   129  of    613.    Elapsed: 0:02:31.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:09.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:02.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:40.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:04.\n",
      "Train labeled Loss: 0.0702\n",
      "Soft labeled Loss: 4.9748\n",
      "Total Loss: 5.0450\n",
      "validation performance at epoch:  0 tensor(0.8407) 0.8025035035998821 0.7900429766495508 0.791652760797733 0.8378400048753742\n",
      "test model performance at epoch :  0 tensor(0.7805) 0.758503526150585 0.722299196431732 0.7336381500446072 0.7774962922731367\n",
      "Epoch: 1\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8343\n",
      "  Batch   129  of    613.    Elapsed: 0:02:30.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:08.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:46.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:24.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:01.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:39.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:17.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:55.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:33.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:11.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:49.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:27.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:05.\n",
      "Train labeled Loss: 0.0312\n",
      "Soft labeled Loss: 4.9632\n",
      "Total Loss: 5.0450\n",
      "validation performance at epoch:  1 tensor(0.8407) 0.8007705582633072 0.8017384857903156 0.793969827447804 0.8378291493727877\n",
      "test model performance at epoch :  1 tensor(0.7846) 0.7633897986071899 0.7634865456678618 0.7612499126951351 0.7838610012102346\n",
      "Epoch: 2\n",
      "613\n",
      "  Batch    40  of     89.    Elapsed: 0:00:50.\n",
      "  Batch    80  of     89.    Elapsed: 0:01:40.\n",
      "Train Accuracy: 0.8989\n",
      "  Batch   129  of    613.    Elapsed: 0:02:29.\n",
      "  Batch   169  of    613.    Elapsed: 0:03:07.\n",
      "  Batch   209  of    613.    Elapsed: 0:03:45.\n",
      "  Batch   249  of    613.    Elapsed: 0:04:23.\n",
      "  Batch   289  of    613.    Elapsed: 0:05:00.\n",
      "  Batch   329  of    613.    Elapsed: 0:05:38.\n",
      "  Batch   369  of    613.    Elapsed: 0:06:16.\n",
      "  Batch   409  of    613.    Elapsed: 0:06:53.\n",
      "  Batch   449  of    613.    Elapsed: 0:07:31.\n",
      "  Batch   489  of    613.    Elapsed: 0:08:09.\n",
      "  Batch   529  of    613.    Elapsed: 0:08:47.\n",
      "  Batch   569  of    613.    Elapsed: 0:09:24.\n",
      "  Batch   609  of    613.    Elapsed: 0:10:02.\n",
      "Train labeled Loss: 0.0194\n",
      "Soft labeled Loss: 4.9620\n",
      "Total Loss: 4.9944\n",
      "validation performance at epoch:  2 tensor(0.8407) 0.8087317371793059 0.8076860391831243 0.8022094178802124 0.8374743500962668\n",
      "test model performance at epoch :  2 tensor(0.7846) 0.7620379155736636 0.7634865456678618 0.7604492525527176 0.7841936449163722\n"
     ]
    }
   ],
   "source": [
    "total_epoch = 3\n",
    "batch_size = 16\n",
    "batch_size_unlabel = 16 * 2\n",
    "T = 10 # temperature\n",
    "unlabel_iter = None\n",
    "model_name = \"paper_student_distillv2_errortrial_epoch\" + str(total_epoch) + \"_batchunlabel\" + str(batch_size_unlabel) + \"_T\" + str(T) + \"_unlabeliter\" + str(unlabel_iter)\n",
    "\n",
    "for i in range(5):\n",
    "    train_distill_student(\"bertweetcovid_paper.pth\", str(i)+model_name, train, unlabeled_prob, val, test, batch_size, batch_size_unlabel, total_epoch, labels_in_dst, learning_rate, T, unlabel_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f52814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c796bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
