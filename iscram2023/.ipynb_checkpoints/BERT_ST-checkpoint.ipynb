{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# Unique to this file\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "from train_test import get_device, compute_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# torch.device(\"mps\")\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "model_map= {\n",
    "    #\"tinybert\": \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "    #\"covidbert\": \"digitalepidemiologylab/covid-twitter-bert-v2\",\n",
    "    #\"distilbert\": \"distilbert-base-uncased\",\n",
    "    #\"bertweet\": \"vinai/bertweet-base\",\n",
    "    \"bertweetcovid\": \"vinai/bertweet-covid19-base-uncased\"\n",
    "}\n",
    "\n",
    "\n",
    "# Depending on your GPU you can either increase or decrease this value\n",
    "batch_size = 16\n",
    "total_epoch = 10\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "# Find out how many labels are in the dataset\n",
    "with open('covid_dataset/5_class_map.pkl','rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "labels_in_dst = len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train teacher model, to reproduce paper result\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "dst_path = 'preprocessed_data/{}.dst'\n",
    "dst_path = os.path.join(cwd,'preprocessed_data/{}-{{}}.dst'.format(\"bertweetcovid\"))\n",
    "\n",
    "train = torch.load(dst_path.format('train'))\n",
    "val = torch.load(dst_path.format('val'))\n",
    "test = torch.load(dst_path.format('test'))\n",
    "unlabeled = torch.load(dst_path.format('19k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/akash/Documents/capstone_project/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/Users/akash/Documents/capstone_project/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1576: UserWarning: torch.cumsum supported by MPS on MacOS 13+, please upgrade (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/UnaryOps.mm:426.)\n",
      "  incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "from train_test import train_paper\n",
    "# reproduce paper results\n",
    "train_paper(train, val, test, batch_size, total_epoch, labels_in_dst, learning_rate, model_path=\"bertweetcovid_paper.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from train_test import test_ST\n",
    "# test best val_weighted f1 model on test\n",
    "_, _ = test_ST(\"bertweetcovid_paper.pth\", test, labels_in_dst, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test import test_ST\n",
    "# validation performance: best val_weighted f1 model \n",
    "_, _ = test_ST(\"bertweetcovid_paper.pth\", val, labels_in_dst, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows sample weight for each training example, when calculate loss \n",
    "def train_teacher(train, val,test, batch_size, total_epoch, labels_in_dst, learning_rate, self_train, model_path):\n",
    "    global model_map\n",
    "\n",
    "    trainloader = DataLoader(train, shuffle=True, batch_size=batch_size)\n",
    "    valloader = DataLoader(val, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    device = get_device()\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_map['bertweetcovid'], \n",
    "                                                          num_labels=labels_in_dst,\n",
    "                                                          return_dict=True)\n",
    "    model = model.to(device)\n",
    "    gc.collect()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    sum_loss = []\n",
    "    sum_val = []\n",
    "    \n",
    "    val_f1_average = []\n",
    "\n",
    "    for epoch in range(0, total_epoch):\n",
    "        print('Epoch:', epoch)\n",
    "        train_loss, valid_loss = [], []\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        step = 0\n",
    "        total_train_accuracy = 0\n",
    "        \n",
    "        for input_ids, attention_mask, labels, sample_weight in trainloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            sample_weight = sample_weight.to(device)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output1 = model(input_ids, attention_mask=attention_mask)\n",
    "            if not self_train:\n",
    "                loss = F.cross_entropy(output1.logits, labels)\n",
    "            else:\n",
    "                logits = output1.logits\n",
    "                # redefine loss that including the sample weights\n",
    "                # loss = BCEWithLogitsLoss(logits, b_labels, weight=b_sample_weights)\n",
    "                loss_fct = CrossEntropyLoss(reduction='none')\n",
    "                loss = loss_fct(logits.view(-1, labels_in_dst), lables.view(-1))\n",
    "                loss = torch.mean(loss * sample_weights) # element wise multiplication\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        sum_loss.append(sum(train_loss)/len(train))  \n",
    "        print('Loss: {:.4f}'.format(sum_loss[epoch-1]))\n",
    "\n",
    "#       evaluation part \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for input_ids, attention_mask, labels in valloader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model(input_ids, attention_mask=attention_mask)\n",
    "                predictions.append(output.logits.clone().detach())\n",
    "                true_labels.append(labels.clone().detach())\n",
    "            predictions = torch.cat(predictions)\n",
    "            true_labels = torch.cat(true_labels)\n",
    "            predictions = predictions.cpu()\n",
    "            true_labels = true_labels.cpu()\n",
    "\n",
    "            # val_f1 is weighted f1 \n",
    "            acc, precision, recall, f1_macro, val_f1  = compute_accuracy(predictions, true_labels)\n",
    "            print(\"validation performance: \", acc, precision, recall, f1_macro, val_f1)\n",
    "            \n",
    "            \n",
    "            best_f1 = max(val_f1_average, default=-1)\n",
    "            best_model_state = ''\n",
    "            # Save the best model seen so far\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_f1': best_f1\n",
    "                            }, model_path)\n",
    "            \n",
    "            val_f1_average.append(val_f1)\n",
    "    \n",
    "        # test\n",
    "        model.eval()\n",
    "        testloader = DataLoader(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            pred_prob = []\n",
    "            for input_ids, attention_mask, labels in testloader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                output = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = output.logits.clone().detach()\n",
    "\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(labels.clone().detach())\n",
    "\n",
    "                softmax = torch.nn.Softmax(dim=1)\n",
    "                prob_batch = softmax(logits)\n",
    "                prob_batch = prob_batch.cpu().numpy()\n",
    "                pred_prob.append(prob_batch)\n",
    "\n",
    "            predictions = torch.cat(predictions)\n",
    "            true_labels = torch.cat(true_labels)\n",
    "            predictions = predictions.cpu()\n",
    "            true_labels = true_labels.cpu()\n",
    "\n",
    "            flat_prob = np.concatenate(pred_prob, axis=0)\n",
    "\n",
    "            pred_labels = np.argmax(flat_prob, axis=1).flatten()\n",
    "\n",
    "            acc, precision, recall,f1_macro, f1_score  = compute_accuracy(predictions, true_labels)\n",
    "\n",
    "            print(\"test model performance after all epochs: \", acc, precision, recall,f1_macro, f1_score)\n",
    "#     last_epoch_model_path = \"hardlabel_lastepoch_\" + model_path\n",
    "#     torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'best_f1': best_f1\n",
    "#                 }, last_epoch_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train teacher model for self training\n",
    "# the train_st.dst is TensorDataset that assign each tweet a weight 1, that's the only difference with train.dst\n",
    "\n",
    "dst_path = 'preprocessed_data/{}.dst'\n",
    "dst_path = os.path.join(cwd,'preprocessed_data/{}-{{}}.dst'.format(\"bertweetcovid\"))\n",
    "\n",
    "train = torch.load(dst_path.format('train_st'))\n",
    "val = torch.load(dst_path.format('val'))\n",
    "test = torch.load(dst_path.format('test'))\n",
    "unlabeled = torch.load(dst_path.format('19k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_labels, pred_probs = test_ST(\"bertweetcovid_paper.pth\", unlabeled, labels_in_dst, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_19k = pd.read_csv('covid_dataset/splits/19k_unlabeled.csv_cleaned.csv')\n",
    "print(len(df_19k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = pd.DataFrame(pred_probs, columns=['class0', 'class1', 'class2', 'class3', 'class4'])\n",
    "# print(len(df_temp))\n",
    "\n",
    "# df_19k = pd.concat([df_19k, df_temp], axis=1)\n",
    "# print(df_19k.columns)\n",
    "# df_19k.to_csv('covid_dataset/splits/19k_bertweetcovid_probs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_19k_probs = pd.read_csv('covid_dataset/splits/19k_bertweetcovid_probs.csv')\n",
    "# print(df_19k.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_19k = pd.read_csv('covid_dataset/splits/19k_bertweetcovid_probs.csv')\n",
    "pred_probs = df_19k[['class0', 'class1', 'class2', 'class3', 'class4']].to_numpy()\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to get hard labeled unlabeled data\n",
    "# but maybe can select directly from TensorDataset, then no need to reclean and retokenize selected tweets\n",
    "\n",
    "class0_top100 = pred_probs[:, 0].flatten().argsort()[-500:]\n",
    "print(len(class0_top100))\n",
    "df_19k.loc[class0_top100, 'Label'] = 0\n",
    "df_unlabeled_selected = df_19k.iloc[class0_top100]\n",
    "top500_index = class0_top100.copy()\n",
    "\n",
    "\n",
    "for i in range(1, labels_in_dst):\n",
    "    classi_top100 = pred_probs[:, i].flatten().argsort()[-500:]\n",
    "    df_19k.loc[classi_top100, 'Label'] = i\n",
    "    \n",
    "    top500_index = np.concatenate((top500_index, classi_top100))\n",
    "    df_unlabeled_selected = pd.concat([df_unlabeled_selected, df_19k.iloc[classi_top100]])\n",
    "print(len(top500_index))\n",
    "print(len(df_19k))\n",
    "df_19k.drop(top500_index, inplace=True)\n",
    "df_19k = df_19k.reset_index(drop=True)\n",
    "print(len(df_19k))  \n",
    "\n",
    "pred_probs_top500 = pred_probs[top500_index]\n",
    "print(pred_probs_top500.shape)\n",
    "                                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "#     \"tinybert\": (\"huawei-noah/TinyBERT_General_4L_312D\",512),\n",
    "#     \"covidbert\": (\"digitalepidemiologylab/covid-twitter-bert-v2\",512),\n",
    "#     \"distilbert\": (\"distilbert-base-uncased\",512),\n",
    "#     \"bertweet\": (\"vinai/bertweet-base\",130),\n",
    "    \"bertweetcovid\": (\"vinai/bertweet-covid19-base-uncased\",130)\n",
    "}\n",
    "\n",
    "data_x = df_unlabeled_selected['clean_tweet'].values.tolist()\n",
    "data_y = df_unlabeled_selected['Label'].values.tolist()\n",
    "\n",
    "df_train = pd.read_csv('covid_dataset/splits/train.csv_cleaned.csv')\n",
    "x_train = df_train['clean_tweet'].values.tolist()\n",
    "y_train = df_train['Label'].values.tolist()\n",
    "\n",
    "print(len(data_x))\n",
    "print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_x = x_train + data_x\n",
    "new_train_y = y_train + data_y\n",
    "print(len(new_train_x), len(new_train_y))\n",
    "\n",
    "\n",
    "for model_type,(model_name,sequence_length) in models.items():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=False, normalization = True)\n",
    "\n",
    "    tokenized_x = tokenizer(new_train_x, return_tensors='pt', padding=True, truncation=True, max_length=sequence_length)\n",
    "    x_input_ids = tokenized_x['input_ids']\n",
    "    x_attention_mask = tokenized_x['attention_mask']\n",
    "    labels = torch.tensor(new_train_y, dtype=torch.long)\n",
    "    \n",
    "    # weight is just one for each selected hard labeled unlabeled tweet\n",
    "    sample_weights = torch.ones(len(new_train_y)).type(torch.FloatTensor)\n",
    "    \n",
    "    new_train_dataset = TensorDataset(x_input_ids, x_attention_mask, labels, sample_weights )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "total_epoch = 10\n",
    "for i in range(5):\n",
    "    train_teacher(new_train_dataset, val, test, batch_size, total_epoch, labels_in_dst, learning_rate, self_train=False, model_path=str(i)+\"bertweetcovid_ST_1iter_500each16batch10epochs.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
