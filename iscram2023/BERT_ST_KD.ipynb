{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c5ca539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# Unique to this file\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "from train_test import get_device, compute_accuracy, test_ST\n",
    "\n",
    "\n",
    "# Depending on your GPU you can either increase or decrease this value\n",
    "batch_size = 16\n",
    "total_epoch = 10\n",
    "learning_rate = 1e-5\n",
    "iter_num = 1\n",
    "\n",
    "# Find out how many labels are in the dataset\n",
    "with open('covid_dataset/5_class_map.pkl','rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "labels_in_dst = len(labels)\n",
    "\n",
    "\n",
    "\n",
    "model_map= {\n",
    "    #\"tinybert\": \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "    #\"covidbert\": \"digitalepidemiologylab/covid-twitter-bert-v2\",\n",
    "    #\"distilbert\": \"distilbert-base-uncased\",\n",
    "    #\"bertweet\": \"vinai/bertweet-base\",\n",
    "    \"bertweetcovid\": \"vinai/bertweet-covid19-base-uncased\"\n",
    "}\n",
    "# train teacher model for self training\n",
    "# the train_st.dst is TensorDataset that assign each tweet a weight 1, that's the only difference with train.dst\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "dst_path = 'preprocessed_data/{}.dst'\n",
    "dst_path = os.path.join(cwd,'preprocessed_data/{}-{{}}.dst'.format(\"bertweetcovid\"))\n",
    "\n",
    "train = torch.load(dst_path.format('train_st'))\n",
    "val = torch.load(dst_path.format('val'))\n",
    "test = torch.load(dst_path.format('test'))\n",
    "unlabeled = torch.load(dst_path.format('19k'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f4e1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19591\n"
     ]
    }
   ],
   "source": [
    "input_ids, masks, _ = zip(*unlabeled)\n",
    "\n",
    "input_ids = torch.stack(input_ids)\n",
    "masks = torch.stack(masks)\n",
    "\n",
    "df_19k = pd.read_csv('covid_dataset/splits/19k_bertweetcovid_probs.csv')\n",
    "\n",
    "pred_probs = df_19k[['class0', 'class1', 'class2', 'class3', 'class4']].to_numpy()\n",
    "\n",
    "paper_pred_prob = torch.tensor(pred_probs, dtype=torch.float32)\n",
    "\n",
    "unlabeled_prob = TensorDataset(input_ids, masks, paper_pred_prob )\n",
    "print(unlabeled_prob.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "217aee01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x29465ae30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "875c6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss for two probability distribution\n",
    "# https://discuss.pytorch.org/t/how-should-i-implement-cross-entropy-loss-with-continuous-target-outputs/10720/18\n",
    "def cross_entropy(pred, soft_targets):\n",
    "    logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n",
    "\n",
    "#assuming pred and soft_targets are both Variables with shape (batchsize, num_of_classes), \n",
    "#each row of pred is predicted logits and each row of soft_targets is a discrete distribution.\n",
    "\n",
    "\n",
    "# class CrossEntropyLossForSoftTarget(nn.Module):\n",
    "#     def __init__(self, T=20):\n",
    "#         super(CrossEntropyLossForSoftTarget, self).__init__()\n",
    "#         self.T = T\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "#         self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "#     def forward(self, y_pred, y_gt):\n",
    "#         y_pred_soft = y_pred.div(self.T)\n",
    "#         y_gt_soft = y_gt.div(self.T)\n",
    "#         return -(y_gt_soft)*self.logsoftmax(y_pred_soft).mean().mul(self.T*self.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "70ffbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do KD with unlabeled data on a given model student_path\n",
    "def student_distill(student_path, train, unlabel, val, test, batch_size, total_epoch, labels_in_dst, learning_rate):\n",
    "    global model_map\n",
    "\n",
    "    trainloader = DataLoader(train, shuffle=True, batch_size=batch_size)\n",
    "    valloader = DataLoader(val, shuffle=False, batch_size=batch_size)\n",
    "    unlabel_loader = DataLoader(unlabel, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    device = get_device()\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_map['bertweetcovid'], \n",
    "                                                          num_labels=labels_in_dst,\n",
    "                                                          return_dict=True)\n",
    "    \n",
    "    checkpoint = torch.load(student_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    save_model_path = \"distill_\" + student_path\n",
    "    model = model.to(device)\n",
    "    gc.collect()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    sum_loss = []\n",
    "    sum_val = []\n",
    "    \n",
    "    val_f1_average = []\n",
    "\n",
    "    for epoch in range(0, total_epoch):\n",
    "        print('Epoch:', epoch)\n",
    "        train_loss, valid_loss = [], []\n",
    "        model.train()\n",
    "        for input_ids, attention_mask, labels in unlabel_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device) # teacher predicted probabilities\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output1 = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            logits = output1.logits\n",
    "            # loss on teacher and student, two probability distributions on unlabeled data \n",
    "#             loss_fct = CrossEntropyLossForSoftTarget() \n",
    "            loss = cross_entropy(logits, labels)\n",
    "#             \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        sum_loss.append(sum(train_loss)/len(train))  \n",
    "        print('Loss: {:.4f}'.format(sum_loss[epoch-1]))\n",
    "\n",
    "#       evaluation part \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for input_ids, attention_mask, labels in valloader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model(input_ids, attention_mask=attention_mask)\n",
    "                predictions.append(output.logits.clone().detach())\n",
    "                true_labels.append(labels.clone().detach())\n",
    "            predictions = torch.cat(predictions)\n",
    "            true_labels = torch.cat(true_labels)\n",
    "            predictions = predictions.cpu()\n",
    "            true_labels = true_labels.cpu()\n",
    "\n",
    "            # val_f1 is weighted f1 \n",
    "            acc, precision, recall, f1_macro, val_f1  = compute_accuracy(predictions, true_labels)\n",
    "            print(\"validation performance at epoch: \", epoch, acc, precision, recall, f1_macro, val_f1)\n",
    "            \n",
    "            \n",
    "            best_f1 = max(val_f1_average, default=-1)\n",
    "            best_model_state = ''\n",
    "            # Save the best model seen so far\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                torch.save(model.state_dict(), save_model_path)\n",
    "#                 torch.save({\n",
    "#                             'epoch': epoch,\n",
    "#                             'model_state_dict': model.state_dict(),\n",
    "#                             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                             'best_f1': best_f1\n",
    "#                             }, save_model_path)\n",
    "            \n",
    "            val_f1_average.append(val_f1)\n",
    "    \n",
    "        # test\n",
    "        model.eval()\n",
    "        testloader = DataLoader(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            pred_prob = []\n",
    "            for input_ids, attention_mask, labels in testloader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                output = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = output.logits.clone().detach()\n",
    "\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(labels.clone().detach())\n",
    "\n",
    "                softmax = torch.nn.Softmax(dim=1)\n",
    "                prob_batch = softmax(logits)\n",
    "                prob_batch = prob_batch.cpu().numpy()\n",
    "                pred_prob.append(prob_batch)\n",
    "\n",
    "            predictions = torch.cat(predictions)\n",
    "            true_labels = torch.cat(true_labels)\n",
    "            predictions = predictions.cpu()\n",
    "            true_labels = true_labels.cpu()\n",
    "\n",
    "            flat_prob = np.concatenate(pred_prob, axis=0)\n",
    "\n",
    "            pred_labels = np.argmax(flat_prob, axis=1).flatten()\n",
    "\n",
    "            acc, precision, recall,f1_macro, f1_score  = compute_accuracy(predictions, true_labels)\n",
    "\n",
    "            print(\"test model performance at epoch : \", epoch, acc, precision, recall,f1_macro, f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfe191cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ST2(checkpoint_path, test, labels_in_dst, batch_size):\n",
    "    global model_map\n",
    "    test_f1_average = []\n",
    "    test_precision = []\n",
    "    test_recall = []\n",
    "    test_acc = []\n",
    "    test_f1 = []\n",
    "    \n",
    "    device = get_device()  \n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_map['bertweetcovid'], \n",
    "                                                          num_labels=labels_in_dst,\n",
    "                                                          return_dict=True)\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    flat_prob = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    testloader = DataLoader(test, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            pred_prob = []\n",
    "            for input_ids, attention_mask, labels in testloader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                output = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = output.logits.clone().detach()\n",
    "                \n",
    "                predictions.append(logits)\n",
    "                true_labels.append(labels.clone().detach())\n",
    "                \n",
    "                softmax = torch.nn.Softmax(dim=1)\n",
    "                prob_batch = softmax(logits)\n",
    "                prob_batch = prob_batch.cpu().numpy()\n",
    "                pred_prob.append(prob_batch)\n",
    "                \n",
    "            predictions = torch.cat(predictions)\n",
    "            true_labels = torch.cat(true_labels)\n",
    "            predictions = predictions.cpu()\n",
    "            true_labels = true_labels.cpu()\n",
    "            \n",
    "            flat_prob = np.concatenate(pred_prob, axis=0)\n",
    "               \n",
    "            pred_labels = np.argmax(flat_prob, axis=1).flatten()\n",
    "            \n",
    "            acc, precision, recall,f1_macro, f1_score  = compute_accuracy(predictions, true_labels)\n",
    "            test_acc.append(acc)\n",
    "            test_f1_average.append(f1_macro)\n",
    "            test_f1.append(f1_score)\n",
    "            test_precision.append(precision)\n",
    "            test_recall.append(recall)\n",
    "            print(\"test performance: \", acc, precision, recall,f1_macro, f1_score)\n",
    "            \n",
    "    return pred_labels, flat_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b96b51f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7846) 0.7422315972547893 0.7914124393783851 0.7609219783639178 0.7860240176079207\n"
     ]
    }
   ],
   "source": [
    "_, _ = test_ST(\"bertweetcovid_ST_1iter_500eachclass.pth\", test, labels_in_dst, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eedede7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2092\n",
      "validation performance at epoch:  0 tensor(0.8431) 0.8039799433985481 0.8152110187259357 0.8037856581240295 0.8406837024965624\n",
      "test model performance at epoch :  0 tensor(0.8008) 0.7847086983638473 0.7758315591449507 0.779795468947741 0.7999913382314555\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2092\n",
      "validation performance at epoch:  1 tensor(0.8407) 0.8149440523839772 0.8076860391831243 0.8048270818249647 0.8374674418482477\n",
      "test model performance at epoch :  1 tensor(0.7805) 0.7530965391621129 0.754807300384843 0.7517758457517494 0.7788943940146954\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1790\n",
      "validation performance at epoch:  2 tensor(0.8358) 0.8062405840846208 0.7820968135938988 0.78795074976583 0.8322190832144352\n",
      "test model performance at epoch :  2 tensor(0.7927) 0.7820490620490621 0.7696189498490464 0.7732231120720858 0.7922526287990183\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "total_epoch = 3\n",
    "learning_rate = 1e-5\n",
    "\n",
    "student_distill(\"0bertweetcovid_ST_1iter_500each16batch10epochs.pth\", train, unlabeled_prob, val, test, batch_size, total_epoch, labels_in_dst, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af354467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7846) 0.7676145268233876 0.7851251877436876 0.771820959379522 0.7854781940879401\n"
     ]
    }
   ],
   "source": [
    "# run 1: self training only\n",
    "_, _ = test_ST(\"0bertweetcovid_ST_1iter_500each16batch10epochs.pth\", test, labels_in_dst, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "958e9bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.8008) 0.7630293198181128 0.7706211597882099 0.766116890743152 0.8006840247836313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/hongminli/opt/miniconda3/envs/CS497DL/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2145\n",
      "validation performance at epoch:  0 tensor(0.8407) 0.8086891792199138 0.8234869049839902 0.8082826520921758 0.8378682717151439\n",
      "test model performance at epoch :  0 tensor(0.7602) 0.7275469435254778 0.7109646359163155 0.7144578803049798 0.7583184325553198\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2145\n",
      "validation performance at epoch:  1 tensor(0.8333) 0.79493973089836 0.8025029509083789 0.7921659122247448 0.8303014183235032\n",
      "test model performance at epoch :  1 tensor(0.7805) 0.7657868352223192 0.7245501989956661 0.7374041867954911 0.778185721953838\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1791\n",
      "validation performance at epoch:  2 tensor(0.8284) 0.7910670453142362 0.8063343225029405 0.7911375015260743 0.8240810123721346\n",
      "test model performance at epoch :  2 tensor(0.7886) 0.7766147695383078 0.7630546214025276 0.7678594570902263 0.786626093067619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7602) 0.7275469435254778 0.7109646359163155 0.7144578803049798 0.7583184325553198\n"
     ]
    }
   ],
   "source": [
    "# run 2 ST\n",
    "_, _ = test_ST(\"1bertweetcovid_ST_1iter_500each16batch10epochs.pth\", test, labels_in_dst, 16)\n",
    "batch_size = 32\n",
    "total_epoch = 3\n",
    "learning_rate = 1e-5\n",
    "# run 2 distill\n",
    "student_distill(\"1bertweetcovid_ST_1iter_500each16batch10epochs.pth\", train, unlabeled_prob, val, test, batch_size, total_epoch, labels_in_dst, learning_rate)\n",
    "\n",
    "# run 2: ST + KD\n",
    "_, _ = test_ST2(\"distill_1bertweetcovid_ST_1iter_500each16batch10epochs.pth\", test, labels_in_dst, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cdd1382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7764) 0.7472347887516759 0.7388359132813804 0.7409038742252747 0.776251975906057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/hongminli/opt/miniconda3/envs/CS497DL/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2083\n",
      "validation performance at epoch:  0 tensor(0.8407) 0.8078432639916926 0.8078036113185283 0.8020619607904237 0.8378468256517749\n",
      "test model performance at epoch :  0 tensor(0.7967) 0.7754226136546374 0.7729744162878079 0.7694238479577825 0.796745769154633\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2083\n",
      "validation performance at epoch:  1 tensor(0.8456) 0.8185699416190746 0.8096846488459676 0.8087764856085575 0.8430456272153588\n",
      "test model performance at epoch :  1 tensor(0.7764) 0.7488001129305477 0.7435479890565027 0.7441512279445416 0.7758187256041027\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1790\n",
      "validation performance at epoch:  2 tensor(0.8407) 0.8068555170549585 0.8067649408167705 0.8010481797037772 0.8377958363559035\n",
      "test model performance at epoch :  2 tensor(0.7927) 0.7615806284724564 0.7503529328471805 0.7526431335955144 0.7910289509205497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7764) 0.7488001129305477 0.7435479890565027 0.7441512279445416 0.7758187256041027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7724) 0.7336275252525253 0.7754680570235103 0.7399784566493908 0.77370341833292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/hongminli/opt/miniconda3/envs/CS497DL/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2089\n",
      "validation performance at epoch:  0 tensor(0.8358) 0.812198922045271 0.7928712243683095 0.794135000709719 0.8322086864394185\n",
      "test model performance at epoch :  0 tensor(0.7967) 0.7878623188405798 0.7330995787461503 0.7528174510684164 0.7925521475484556\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2089\n",
      "validation performance at epoch:  1 tensor(0.8431) 0.8045084700699728 0.808224794831369 0.8016657308078589 0.8404016978987733\n",
      "test model performance at epoch :  1 tensor(0.7967) 0.7865195693937637 0.7528721623521439 0.7651603251913428 0.794188860862681\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1792\n",
      "validation performance at epoch:  2 tensor(0.8309) 0.8037588692136074 0.7791771055647017 0.7845052798665794 0.826934190073491\n",
      "test model performance at epoch :  2 tensor(0.7805) 0.7687024044929617 0.7275815578806835 0.7419954493078805 0.7772020192168154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7967) 0.7865195693937637 0.7528721623521439 0.7651603251913428 0.794188860862681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7520) 0.6960830527497194 0.7282235527932721 0.7048031067154543 0.7579093201168468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/hongminli/opt/miniconda3/envs/CS497DL/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2093\n",
      "validation performance at epoch:  0 tensor(0.8456) 0.811502798715075 0.8117619898494832 0.8068450563155791 0.8430785898307819\n",
      "test model performance at epoch :  0 tensor(0.7967) 0.7727102216897604 0.7767618069919037 0.772951203385986 0.7969136240610258\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2093\n",
      "validation performance at epoch:  1 tensor(0.8382) 0.80687232141505 0.7895042210013061 0.7925905236236371 0.8348260336575066\n",
      "test model performance at epoch :  1 tensor(0.8089) 0.7950839215665477 0.7936551989198101 0.7895828437502731 0.8082757080579802\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j6/xh538drx43scvlcbq5strhl80000gq/T/ipykernel_59887/1447815066.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1812\n",
      "validation performance at epoch:  2 tensor(0.8407) 0.8115216066965656 0.7928062717486124 0.795444987385633 0.8370870075604978\n",
      "test model performance at epoch :  2 tensor(0.7846) 0.7573653765408226 0.736477094005856 0.7439202353710362 0.7828769697307957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance:  tensor(0.7967) 0.7727102216897604 0.7767618069919037 0.772951203385986 0.7969136240610258\n"
     ]
    }
   ],
   "source": [
    "# run 3-5 ST\n",
    "for i in range(2,5):\n",
    "    _, _ = test_ST(str(i)+\"bertweetcovid_ST_1iter_500each16batch10epochs.pth\", test, labels_in_dst, 16)\n",
    "    batch_size = 32\n",
    "    total_epoch = 3\n",
    "    learning_rate = 1e-5\n",
    "    # run 3-5 distill\n",
    "    student_distill(str(i)+\"bertweetcovid_ST_1iter_500each16batch10epochs.pth\", train, unlabeled_prob, val, test, batch_size, total_epoch, labels_in_dst, learning_rate)\n",
    "\n",
    "    # run 3-5: ST + KD\n",
    "    _, _ = test_ST2(\"distill_\"+str(i)+\"bertweetcovid_ST_1iter_500each16batch10epochs.pth\", test, labels_in_dst, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83974c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
